{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2afe89f1-7451-4d61-88fe-082783325aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras # Need this when working in a Jupyter notebook\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential # Model used to create the architecture both ANNs and CNNs\n",
    "from keras.layers.core import Dense \n",
    "# from keras.optimizers import SGD \n",
    "from tensorflow.keras.optimizers import SGD # Use the above instead in GCP. Colab can use the previous line.\n",
    "from keras.layers.convolutional import Conv2D # For CNNs\n",
    "from keras.layers.convolutional import MaxPooling2D # For CNNs\n",
    "from keras.layers.core import Flatten # For CNNs\n",
    "from keras.layers.core import Activation # For CNNs\n",
    "from keras.layers.core import Dropout # For CNNs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os as os\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from scipy import ndimage\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ea153-d15c-4f43-a15a-de588f6cdbf8",
   "metadata": {},
   "source": [
    "Bucket mount instruction in image preparation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db673fbc-e213-443f-93d7-9e625b70c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'gcs/resize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "969f6c0d-fe68-4e37-b87d-f8b37cf1d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter\n"
     ]
    }
   ],
   "source": [
    "cd ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4253dd84-5abb-4548-90f7-5a42a68941ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dragon_snake', 'flower', 'fox_tiger', 'mix', 'small_animal']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects = os.listdir(dataDir)\n",
    "objects = objects[1:]\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62645bd6-784e-4a4b-b4ca-b2591229b5c0",
   "metadata": {},
   "source": [
    "# Binarizing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b3f01df-2e49-47fe-83d6-0f556b2bbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dragon_snake_files = globFn(objects[0])\n",
    "flower_files = globFn(objects[1])\n",
    "mix_files = globFn(objects[2])\n",
    "fox_tiger_files = globFn(objects[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "830f32c5-77ba-40cb-b41a-590a30dcb99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flower']\n",
      "[[0]]\n",
      "['dragon_snake', 'flower', 'fox_tiger', 'mix', 'small_animal']\n"
     ]
    }
   ],
   "source": [
    "stackedArrayLabels = []\n",
    "stackedArrayLabels.append(objects[1])\n",
    "print(stackedArrayLabels)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "binarizedLabels = lb.fit_transform(stackedArrayLabels)\n",
    "print(binarizedLabels)\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa42aeb-e1d3-48ed-a58f-80b0fa2ab089",
   "metadata": {},
   "source": [
    "The binarized label doesn't work as intended because there's only one image to binarize it's label. If all images were included in the binarization then the binarized labels should look like \n",
    "\n",
    "$\\big[$0, 1, 0, 0, 0$\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6deb332-d5e3-43e4-b2f7-617c6c8a2aae",
   "metadata": {},
   "source": [
    "# ANN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ade41-d26e-4a61-b43f-1735c7cf74c4",
   "metadata": {},
   "source": [
    "Can put the following cell into a function based on data augmentation parameters. First have to learn how to use the data augmentation function from the Keras package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1bc062-4c44-4717-84a9-e304001effff",
   "metadata": {},
   "source": [
    "## normalising and flattening the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9b01458-70e9-4d69-9e7d-80342565273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mIG-Trends-with-Machine-Learning\u001b[0m/  \u001b[01;34mgcs\u001b[0m/  \u001b[01;34msrc\u001b[0m/  \u001b[01;34mtutorials\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "45484ebf-11e8-4fc4-9b65-eeb08c8ee2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcs/resize'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef54680d-10b1-45f0-81d6-575358d5a095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "stackedArrayValues = [] # Normalised, resized and stacked 1D arrays corresponding to the images.\n",
    "stackedArrayLabels = [] # 1D array for the labels.\n",
    "\n",
    "# Finding all the images in each type that starts the object type.\n",
    "for imageType in objects:\n",
    "    image_type_folder = os.path.abspath(dataDir + '/' + imageType)\n",
    "    files = glob.glob(image_type_folder + '/' + imageType + '_*.jpg')\n",
    "\n",
    "# For each source, we want to resize and normalise the image.\n",
    "    for filename in files:\n",
    "        data = plt.imread(filename)\n",
    "        # resizeData = ndimage.zoom(data,zoom=zoom_factor) \n",
    "\n",
    "        # Normalisation is done by first subtracting the minimum value, and then dividing by the maximum value in the subtracted values.\n",
    "        normalisedData = (data-np.min(data))/np.max(data-np.min(data))\n",
    "        normalised1D = normalisedData.flatten() # Turning the 2D image into a 1D array.\n",
    "        stackedArrayValues.append(normalised1D)\n",
    "        stackedArrayLabels.append(imageType)\n",
    "        \n",
    "# Converting the lists into a 1D array.\n",
    "stackedArrayValues = np.array(stackedArrayValues)\n",
    "stackedArrayLabels = np.array(stackedArrayLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be7265b0-59c1-498b-bff5-3354aaf7d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedArrayValues = [] # Normalised, resized and stacked 1D arrays corresponding to the images.\n",
    "stackedArrayLabels = [] # 1D array for the labels.\n",
    "\n",
    "# Finding all the images in each type that starts the object type.\n",
    "for imageType in objects:\n",
    "    image_type_folder = os.path.abspath(dataDir + '/' + imageType)\n",
    "    files = glob.glob(image_type_folder + '/' + imageType + '_*.jpg')\n",
    "\n",
    "# For each source, we want to resize and normalise the image.\n",
    "    for filename in files:\n",
    "        arrayValues = np.array([])\n",
    "        arrayLabels = np.array([])\n",
    "        data = plt.imread(filename)\n",
    "        # resizeData = ndimage.zoom(data,zoom=zoom_factor) \n",
    "\n",
    "        # Normalisation is done by first subtracting the minimum value, and then dividing by the maximum value in the subtracted values.\n",
    "        normalisedData = (data-np.min(data))/np.max(data-np.min(data))\n",
    "        normalised1D = normalisedData.flatten() # Turning the 2D image into a 1D array.\n",
    "        \n",
    "        # arrayValues.append(normalised1D)\n",
    "        arrayValues = np.append(arrayValues, normalised1D)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # When appending to the stacked array, we want to make sure the flattened images have the same dimensions\n",
    "        if arrayValues.shape[0] == 150528:\n",
    "            stackedArrayValues.append(arrayValues)\n",
    "            \n",
    "            # Appending the image types\n",
    "            stackedArrayLabels.append(imageType)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Converting the lists into a 1D array.\n",
    "stackedArrayValues = np.array(stackedArrayValues)\n",
    "stackedArrayLabels = np.array(stackedArrayLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8f2b90d1-d1bb-46c1-8afc-8cda5808fe78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 150528)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stackedArrayValues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c969aaa1-2d1e-4551-b087-3e8d374158ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388,)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stackedArrayLabels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c659d-4c39-4575-aabe-d26935d22790",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Checking if there are flat images that are different in shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1e88c-44ae-4897-8bb8-153af50372be",
   "metadata": {},
   "source": [
    "This has been corrected in the stacked array appending cell above. The flat images of different shapes have simply been excluded. Need to learn why they have different shapes later. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fe60d16-a74b-46b4-b918-e0dc89f7338d",
   "metadata": {},
   "source": [
    "stackedArrayValues.shape[0] < 150528"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fef61-18dd-4f9d-8ce2-375df203176d",
   "metadata": {},
   "source": [
    "Finding the location of the flat images that are different. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbac242f-61c8-48a8-9cda-5ad5163e4c1c",
   "metadata": {},
   "source": [
    "for index, image in enumerate(stackedArrayValues):\n",
    "    if image.shape[0] != 150528:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534cb7cd-c0e4-4ef9-9a94-cebbe6011f0d",
   "metadata": {},
   "source": [
    "Removing the flat images with different shapes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "30c6f212-3bad-406a-9404-7028ccc249b3",
   "metadata": {},
   "source": [
    "arr = np.delete(stackedArrayValues, [124, 125], 0)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1285a9-70a5-4ea0-a025-6e08b52cde4c",
   "metadata": {},
   "source": [
    "## Binarising the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f766b8ed-66db-4c1c-bb4b-a3f62be0b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarising the labels once all the sources for each object were identified, resized and normalised:\n",
    "lb = LabelBinarizer()\n",
    "binarizedLabels = lb.fit_transform(stackedArrayLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5bc13ff5-3888-4f15-99a5-a4e2a5b75028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dragon_snake' 'flower' 'fox_tiger' 'mix' 'small_animal']\n"
     ]
    }
   ],
   "source": [
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44173bf9-614a-44ee-afd5-264e967d7151",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9f8dbfa6-cf8f-40ab-91cf-8db4dc6baf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData, trainLabels, testLabels = train_test_split(stackedArrayValues, binarizedLabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9b52b3ed-fefa-4e80-b080-57921c60282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trainData: (271, 150528)\n",
      "Shape of testData: (117, 150528)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of trainData: {trainData.shape}\")\n",
    "print(f\"Shape of testData: {testData.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40969e35-4624-430d-9164-e1c1efa19eb9",
   "metadata": {},
   "source": [
    "## Running the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6d6f6d25-b014-49b9-8579-1ee9154c8813",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3402/1028257179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#applying the data to the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m H = model.fit(trainData, trainLabels, validation_data=(testData, testLabels),\n\u001b[0;32m---> 30\u001b[0;31m               epochs=epochs, batch_size=64, verbose = 0 )\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() #Sequential is the model, so defining it in a variable so we can use it later.\n",
    "\n",
    "model.add(Dense(256, input_shape=(150528,), activation=\"sigmoid\"))\n",
    "#Sigmoid activates the layer.\n",
    "#first parameter of Dense() is the nodes, we used these numbers in the lectorial. Second parameter is the shape of the image\n",
    "\n",
    "model.add(Dense(128, activation=\"sigmoid\"))#hidden layer\n",
    "\n",
    "model.add(Dense(3, activation=\"softmax\"))#first parameter is how many classifications we have. \n",
    "#we have three layers, one input layer, one hidden layer and one output layer. \n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "learnRate = 0.01 #one of the most important parameters to set in the model. \n",
    "#learnRate is for determining how much to change a weight to drive the loss function to a minimum. \n",
    "#higher it's set = faster the model runs, but less accurate it is. \n",
    "\n",
    "opt = SGD(learnRate,momentum=0.9) #used for optimising the model. \n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])#compiling the model\n",
    "\n",
    "epochs = 100 #number of runs the model does. The more epochs can cause overfit and too little epochs will be underfit. \n",
    "\n",
    "#applying the data to the model. \n",
    "H = model.fit(trainData, trainLabels, validation_data=(testData, testLabels),\n",
    "              epochs=epochs, batch_size=64, verbose = 0 )\n",
    "\n",
    "\n",
    "#applying the model to all the test data to check how well it's performed. \n",
    "predictionsANN = model.predict(testData)\n",
    "\n",
    "# Print a formatted report\n",
    "print(classification_report(testLabels.argmax(axis=1), \n",
    "                                                  \n",
    "                            predictionsANN.argmax(axis=1), \n",
    "                            target_names=[str(x) for x in lb.classes_]))#sets the labels for each output\n",
    "                                                                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f8773-e6cf-44a8-a948-c6da2cfc3fb2",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860040d-5a76-4c62-ad56-f5a107c1c429",
   "metadata": {},
   "source": [
    "for cnn have to consider the astro objects had 3 different wavelenghts for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371008f7-da9b-48ee-9f58-8827ae24610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnPrepFn(directory, source_type, rotation_factor):\n",
    "  # Similar initial process as the ANN function:\n",
    "  files = glob.glob(directory + '/' + source_type+'/'+'*_I4.fits')\n",
    "  # The amount of zooming for the images will impact the image's dimensions.\n",
    "  # Since 302x302 is the original size of the images, we want the dimension of the zoomed-in images which depends on the zoom factor.\n",
    "  imagesArr = np.zeros((100,int(zoom_factor*302),int(zoom_factor*302),3)) \n",
    "  labelsList = [] # The labels will become a 1D array.\n",
    "\n",
    "  # Identifying the 3 images that will be resized and normalised:\n",
    "  for indx in np.arange(0,len(files)):\n",
    "    source = files[indx].split('_I4.fits')[0] # Finding the name for each source by splitting the string from `glob.glob` into two parts.\n",
    "                                              # We don't want the text starting from _, so we use [0] to get the source name only.\n",
    "    i2Image = plt.imread(source+'_I2.fits') # The I2 image for the source.\n",
    "\n",
    "    # Resizing the three Spitzer-band images above and finding the maximum value in each image:\n",
    "    resizeI2 = ndimage.rotate(i2Image, rotation_factor, reshape = False)\n",
    "    resizeI2Max = np.max(resizeI2)\n",
    "\n",
    "\n",
    "    # Normalisation is done by finding the greatest value in the maxima for each of the 3 images.\n",
    "    # This is so all the values are smaller than or equal to 1 (for the CNN), and maintain the colour differences between them.\n",
    "    normFactor = np.max([resizeI2Max, resizeI3Max, resizeI4Max])\n",
    "\n",
    "    # Applying the normalisation factor to the resized images:\n",
    "    normI2 = resizeI2/normFactor\n",
    "\n",
    "    # Adding the resized and normalised images into the iamge array:\n",
    "    imagesArr[indx,:,:,0] = normI2\n",
    "\n",
    "    # Adding the label that identifies the source type (HII, PNE, RG):\n",
    "    labelsList.append(source_type)\n",
    "\n",
    "  # Converting the list of labels into an array:\n",
    "  labelsArr = np.array(labelsList)\n",
    "\n",
    "  return imagesArr, labelsArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d20428-cdf8-4818-9161-afdf657ee158",
   "metadata": {},
   "outputs": [],
   "source": [
    "HII_Images_0, HII_Labels_0 = cnnPrepFn2(dir, \"HII\", zoom_factor=0.6, rotation_factor= 0)\n",
    "PNE_Images_0, PNE_Labels_0 = cnnPrepFn2(dir, \"PNE\", zoom_factor=0.6, rotation_factor= 0)\n",
    "RG_Images_0, RG_Labels_0 = cnnPrepFn2(dir, \"RG\", zoom_factor=0.6, rotation_factor= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07b23f-d6db-433d-ba74-18217c46ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_img_aug = np.concatenate((HII_Images_0, HII_Images_90, HII_Images_180, HII_Images_270, PNE_Images_0, PNE_Images_90, PNE_Images_180, PNE_Images_270, RG_Images_0, RG_Images_90, RG_Images_180, RG_Images_270), axis = 0)\n",
    "combined_labels_aug = np.concatenate((HII_Labels_0, HII_Labels_90, HII_Labels_180, HII_Labels_270, PNE_Labels_0, PNE_Labels_90, PNE_Labels_180, PNE_Labels_270, RG_Labels_0, RG_Labels_90, RG_Labels_180, RG_Labels_270), axis = 0)\n",
    "\n",
    "combined_img_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792b58d-3b36-4c2c-8138-cc4e76fa9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarising the labels for the CNN:\n",
    "lb = LabelBinarizer()\n",
    "combined_labels_aug = lb.fit_transform(combined_labels_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c74f41-b58f-46f0-a9ef-386fb371d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnFn(combinedImages, combinedLabels, layers: int, neurons: float, neuronsDescending: bool(), numEpochs: int, learnRate: float, momentum: float):\n",
    "  '''\n",
    "  layers = number of layers to add\n",
    "\n",
    "  neurons = integer from 0.5 to 2.0. Either halving or doubling the number of neurons. \n",
    "\n",
    "  numEpochs = number of epochs to train the model. \n",
    "  '''\n",
    "  \n",
    "  trainData, testData, trainLabels, testLabels = train_test_split(combinedImages, combinedLabels, test_size=0.3)\n",
    "\n",
    "  modelCNN = Sequential()\n",
    "  size = trainData.shape[1]\n",
    "  inputShape = (size, size, 3)\n",
    "\n",
    "  chanDim = -1\n",
    "  if K.image_data_format() == \"channels_first\":\n",
    "    inputShape = (size, size, 3)\n",
    "    chanDim = 1\n",
    "  \n",
    "  neurons = [neurons, neurons]\n",
    "\n",
    "  modelCNN.add(Conv2D(32*neurons[0], (3,3), input_shape = inputShape))\n",
    "  modelCNN.add(Activation('relu'))\n",
    "  modelCNN.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "  modelCNN.add(Conv2D(32*neurons[0], (3,3)))\n",
    "  modelCNN.add(Activation('relu'))\n",
    "  modelCNN.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "  if neuronsDescending == True:\n",
    "    neurons = [neurons[0]*0.5, neurons[1]*0.25]\n",
    "\n",
    "  modelCNN.add(Conv2D(64*neurons[0], (3,3))) \n",
    "  modelCNN.add(Activation('relu'))\n",
    "  modelCNN.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "  if layers == 1:\n",
    "    modelCNN.add(Conv2D(64*neurons[1], (3,3))) \n",
    "    modelCNN.add(Activation('relu'))\n",
    "    modelCNN.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "  if layers == 2:\n",
    "    modelCNN.add(Conv2D(64*neurons[1], (3,3))) \n",
    "    modelCNN.add(Activation('relu'))\n",
    "    modelCNN.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "  modelCNN.add(Flatten()) \n",
    "  modelCNN.add(Dense(64))\n",
    "  modelCNN.add(Activation('relu'))\n",
    "  modelCNN.add(Dropout(0.5))\n",
    "\n",
    "  nClasses = 3\n",
    "  modelCNN.add(Dense(nClasses))\n",
    "  modelCNN.add(Activation('softmax'))\n",
    "\n",
    "  # Optimiser and compiler\n",
    "  opt = SGD(learning_rate= learnRate, momentum= momentum)\n",
    "\n",
    "  modelCNN.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "  batchSize = 32\n",
    "\n",
    "  # Running the CNN model\n",
    "  H = modelCNN.fit(trainData, trainLabels, \n",
    "                validation_data=(testData, testLabels),\n",
    "                batch_size= batchSize, epochs = numEpochs, verbose= 0)\n",
    "\n",
    "\n",
    "  # Plot the train/valid loss and train/valid accuracy \n",
    "  plot_train_curves(H.history)\n",
    "\n",
    "  # Modelling all the test data\n",
    "  predictionsCNN = modelCNN.predict(testData)\n",
    "  yPred = predictionsCNN.argmax(axis=1)\n",
    "\n",
    "  # Printing classification report\n",
    "  report = classification_report(testLabels.argmax(axis=1),\n",
    "                                yPred,\n",
    "                                target_names=[\"HII\",\"PNE\",\"RG\"])\n",
    "  print(report)\n",
    "\n",
    "  cmatrix = confusion_matrix(testLabels.argmax(axis=1), yPred)\n",
    "  plot_confusion_matrix(cmatrix, classes = ['HII', 'PNE', 'RG'],\n",
    "                        title = 'Classification Confusion Matrix', normalize=True)\n",
    "  print(cmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77106b37-58ad-430e-8c3e-980c71f0e03e",
   "metadata": {},
   "source": [
    "# fastai and pytorch comparison"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
